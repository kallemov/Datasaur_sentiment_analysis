{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT model for multi-class sentiment analysis\n",
    "\n",
    "Bak Kallemov, Insight AI Fellowship programm AICV20B, June 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import metrics\n",
    "\n",
    "import random\n",
    "seed_val = 1\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "max_length=300\n",
    "polarity_dict={'positive':0, 'negative':1, 'neutral':2}\n",
    "inverse_dict={y:x for x,y in polarity_dict.items()}\n",
    "saved_model='notebook_data/finetuned_BERT_epoch_10.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/polarity3_data/twitter_polarity3.csv', encoding='latin-1')\n",
    "df['label'] = df.sentiment.replace(polarity_dict)\n",
    "#df=df.sample(n=10000,axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(polarity_dict),\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=False)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(saved_model):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(polarity_dict),\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(saved_model, map_location=torch.device('cuda')))\n",
    "    return model\n",
    "\n",
    "def train(dataloader_train, dataloader_validation):\n",
    "    if not os.path.isdir('checkpoints'): \n",
    "        os.mkdir('checkpoints')\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "\n",
    "        torch.save(model.state_dict(), f'checkpoints/finetuned_BERT_epoch_{epoch}.model')\n",
    "\n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "        val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        \n",
    "        tqdm.write(f'F1 Score (Weighted): {metrics.f1_score_func(predictions, true_vals)}')\n",
    "        tqdm.write(metrics.classification_report_func(predictions, true_vals,polarity_dict.keys()))\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "def predict(inputs):\n",
    "\n",
    "    model.eval()\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "    encoded_data_predict = tokenizer.batch_encode_plus(\n",
    "        inputs, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoded_data_predict.to(device)\n",
    "    input_ids_predict = encoded_data_predict['input_ids']\n",
    "    attention_masks_predict = encoded_data_predict['attention_mask']\n",
    "\n",
    "    dataset_predict = TensorDataset(input_ids_predict, attention_masks_predict)\n",
    "    dataloader_predict = DataLoader(dataset_predict, \n",
    "                                    sampler=SequentialSampler(dataset_predict), \n",
    "                                    batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    predictions = []\n",
    "    total_score=[0]*len(polarity_dict)\n",
    "    for batch in dataloader_predict:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            scores=torch.max(torch.nn.Softmax(dim=1)(outputs[0]),axis=1)\n",
    "            for i,x in enumerate(scores[0]):\n",
    "                ind=int(scores[1][i])\n",
    "                predictions.append((ind,round(float(x),4)))\n",
    "                total_score[ind] +=x\n",
    "    return predictions, total_score, outputs\n",
    "    \n",
    "def predict_single(sentense):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    encoded_data_test = tokenizer.encode_plus(\n",
    "    sentense, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    encoded_data_test.to(device)\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded_data_test)\n",
    "    return torch.max(torch.nn.Softmax(dim=1)(outputs[0]),axis=1), encoded_data_test['input_ids'], outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "max_length=300\n",
    "polarity_dict={'positive':0, 'negative':1, 'neutral':2}\n",
    "inverse_dict={y:x for x,y in polarity_dict.items()}\n",
    "saved_model='notebook_data/finetuned_BERT_epoch_10.model'\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.text.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.10, \n",
    "                                                  random_state=1, \n",
    "                                                  stratify=df.label.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_val,\n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_val)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataloader_train, dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "report=metrics.classification_report_func(predictions, true_vals, polarity_dict.keys())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "cm = confusion_matrix(labels_flat, preds_flat)\n",
    "df_cm = pd.DataFrame(cm, index=polarity_dict.keys(), columns=polarity_dict.keys())\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=['Imagine all the people living life in peace',\n",
    "        'I love this restaurant but the line is too long',\n",
    "        'we should be making a profit by this time next year.',\n",
    "        'She earned her PhD in physics before becoming a postdoctoral fellow at Rockefeller University, where she worked on developing and implementing an underwater touchscreen for dolphins.',\n",
    "        'I did smile today!',\n",
    "        'Applies the Softmax function to an n-dimensional input Tensor', \n",
    "        'I have not slept enough today', \n",
    "        'the weather is crazy hot today',\n",
    "        'Listen Morty, I hate to break it to you, but what people call “love” is just a chemical reaction that compels animals to breed. It hits hard, Morty, then it slowly fades, leaving you stranded in a failing marriage. I did it. Your parents are gonna do it. Break the cycle, Morty. Rise above. Focus on science.',\n",
    "        'I am a scientist, because I invent, transform, create, and destroy for a living, and when I do not like something about the world, I change it',\n",
    "        'I turned myself into a pickle, Morty! I’m Pickle Rick!',\n",
    "        'Do you wanna develop an app?',\n",
    "        'Honey, stop raising your father cholesterol so you can take a hot funeral selfie.',\n",
    "        'To live is to risk it all; otherwise you are just an inert chunk of randomly assembled molecules drifting wherever the universe blows you...'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(saved_model)\n",
    "scores, overall_score,outputs = predict(inputs)\n",
    "[(inverse_dict[int(s[0])],round(float(s[1]),3), inputs[i][:50]+ ('...' if len(inputs[i])>50 else '')) for i,s in enumerate(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "encoding = tokenizer.encode_plus(\n",
    "  inputs[0],\n",
    "  max_length=50,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=True,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "out=tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "pad_index=out.index('[PAD]')\n",
    "out[:pad_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('test.json')\n",
    "df=pd.DataFrame(df['sentences'].to_list())\n",
    "inputs=df.text.values\n",
    "scores, overall_score = predict(inputs)\n",
    "[(inverse_dict[int(s[0])],round(float(s[1]),3), inputs[i][:50]+ ('...' if len(inputs[i])>50 else '')) for i,s in enumerate(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, input_ids, output = predict_single('Today is not an awesome day one two three')\n",
    "inverse_dict[int(score[1])],round(float(score[0]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "pad_index=out.index('[PAD]')\n",
    "print(pad_index,out[:pad_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1][0][:,:pad_index,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPTUM experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization\n",
    "\n",
    "\n",
    "import random\n",
    "seed_val = 1\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=300\n",
    "polarity_dict={'positive':0, 'negative':1, 'neutral':2}\n",
    "inverse_dict={y:x for x,y in polarity_dict.items()}\n",
    "saved_model='notebook_data/finetuned_BERT_epoch_10.model'\n",
    "sentence='Imagine all the people living life in peace.'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def load_model(saved_model):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(polarity_dict),\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(saved_model, map_location=torch.device('cuda')))\n",
    "    return model\n",
    "def predict_single(sentense):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    encoded_data_test = tokenizer.encode_plus(\n",
    "    sentense, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=False, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    encoded_data_test.to(device)\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded_data_test)\n",
    "    return torch.max(torch.nn.Softmax(dim=1)(outputs[0]),axis=1), encoded_data_test['input_ids'], outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bert_outputs(model_bert, embedding_output, attention_mask=None, head_mask=None):\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones(embedding_output.shape[0], embedding_output.shape[1]).to(embedding_output)\n",
    "\n",
    "    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    extended_attention_mask = extended_attention_mask.to(dtype=next(model_bert.parameters()).dtype) # fp16 compatibility\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "    if head_mask is not None:\n",
    "        if head_mask.dim() == 1:\n",
    "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            head_mask = head_mask.expand(model_bert.config.num_hidden_layers, -1, -1, -1, -1)\n",
    "        elif head_mask.dim() == 2:\n",
    "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
    "        head_mask = head_mask.to(dtype=next(model_bert.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
    "    else:\n",
    "        head_mask = [None] * model_bert.config.num_hidden_layers\n",
    "\n",
    "    encoder_outputs = model_bert.encoder(embedding_output,\n",
    "                                         extended_attention_mask,\n",
    "                                         head_mask=head_mask)\n",
    "    sequence_output = encoder_outputs[0]\n",
    "    sequence_output.to(device)\n",
    "    pooled_output = model_bert.pooler(sequence_output)\n",
    "    pooled_output.to(device)\n",
    "    outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "    return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)    \n",
    "\n",
    "\n",
    "class BertModelWrapper(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(BertModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, embeddings):        \n",
    "        outputs = compute_bert_outputs(self.model.bert, embeddings)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        #pooled_output = self.model.dropout(pooled_output)\n",
    "        logits = self.model.classifier(pooled_output)\n",
    "        return torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_wrapper = BertModelWrapper(model)\n",
    "bert_model_wrapper.to(device)\n",
    "ig = IntegratedGradients(bert_model_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores,input_ids,output = predict_single(sentence)\n",
    "input_embedding=output[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=  torch.Size([1, 11]) torch.Size([1, 11, 768])\n",
      "embedding time= 0.021704673767089844\n",
      "ig time =  0.17499494552612305\n",
      "['imagine', 'all', 'the', 'people', 'living', 'life', 'in', 'peace', '.']\n",
      "[ 0.          1.          0.29191798 -0.05807778 -0.24827234 -0.1239868\n",
      " -0.27984333 -0.01044027  0.23614582  0.22888951  0.        ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.32)</b></text></td><td><text style=\"padding-right:2em\"><b>label</b></text></td><td><text style=\"padding-right:2em\"><b>1.04</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> imagine                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> people                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> living                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> peace                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(model_wrapper, sentence, label=0):\n",
    "    start = time.time()\n",
    "    '''\n",
    "    model_wrapper.eval()\n",
    "    model_wrapper.zero_grad()\n",
    "    \n",
    "    #input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "    encoded_data = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=False, \n",
    "        max_length=300, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoded_data.to(device)\n",
    "    input_ids=encoded_data['input_ids']\n",
    "    \n",
    "    input_ids.to(device)\n",
    "    input_embedding = model_wrapper.model.bert.embeddings(encoded_data['input_ids'])\n",
    "    '''\n",
    "    print('shape= ', input_ids.shape, input_embedding.shape)\n",
    "    # predict\n",
    "    pred = model_wrapper(input_embedding).item()\n",
    "    pred_ind = round(pred)\n",
    "    end = time.time()\n",
    "    print('embedding time=', end - start)\n",
    "    start=time.time()\n",
    "    \n",
    "    # compute attributions and approximation delta using integrated gradients\n",
    "    attributions_ig, delta = ig.attribute(input_embedding, n_steps=100, return_convergence_delta=True)\n",
    "    end = time.time()\n",
    "    print('ig time = ', end - start)\n",
    "    \n",
    "    #print('pred: ', pred_ind, '(', '%.2f' % pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu().numpy().tolist())    \n",
    "    print(tokens[1:-1])\n",
    "    #add_attributions_to_visualizer(attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "    attributions=add_attributions_to_visualizer(attributions_ig, tokens, \n",
    "                                   pred, pred_ind, label, delta, \n",
    "                                   vis_data_records_ig)\n",
    "   \n",
    "    return tokens[1:-1], attributions    \n",
    "def add_attributions_to_visualizer(attributions, tokens, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.detach().cpu().numpy()\n",
    "    attributions[0]=0\n",
    "    attributions[-1]=0\n",
    "    attributions /=max(attributions)\n",
    "    print(attributions)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions[1:-1],\n",
    "                            pred,\n",
    "                            pred_ind,\n",
    "                            label,\n",
    "                            \"label\",\n",
    "                            attributions.sum(),       \n",
    "                            tokens[1:-1],\n",
    "                            delta\n",
    "                            ))\n",
    "    return attributions\n",
    "    \n",
    "tokens, attributions=interpret_sentence(bert_model_wrapper, sentence=sentence, label=0)\n",
    "atts=visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_special_tokens(token):\n",
    "    #if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "    #    return \"#\" + token.strip(\"<>\")\n",
    "    return token\n",
    "\n",
    "def _get_color(attr):\n",
    "    # clip values to prevent CSS errors (Values should be from [-1,1])\n",
    "    attr = max(-1, min(1, attr))\n",
    "    if attr > 0:\n",
    "        hue = 120\n",
    "        sat = 75\n",
    "        lig = 100 - int(50 * attr)\n",
    "    else:\n",
    "        hue = 0\n",
    "        sat = 75\n",
    "        lig = 100 - int(-40 * attr)\n",
    "    return \"hsl({}, {}%, {}%)\".format(hue, sat, lig)\n",
    "\n",
    "def format_word_importances(words, importances):\n",
    "    if importances is None or len(importances) == 0:\n",
    "        return \"<td></td>\"\n",
    "    assert len(words) <= len(importances)\n",
    "    tags = [\"<td>\"]\n",
    "    for word, importance in zip(words, importances[: len(words)]):\n",
    "        word = format_special_tokens(word)\n",
    "        color = _get_color(importance)\n",
    "        unwrapped_tag = '<mark style=\"background-color: {color}; opacity:1.0; \\\n",
    "                    line-height:1.75\"><font color=\"black\"> {word}\\\n",
    "                    </font></mark>'.format(\n",
    "            color=color, word=word\n",
    "        )\n",
    "        tags.append(unwrapped_tag)\n",
    "    tags.append(\"</td>\")\n",
    "    return \"\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><th>Word Importance</th><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> imagine                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> people                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> living                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> peace                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "dom = [\"<table width: 100%>\"]\n",
    "rows = [\"<th>Word Importance</th>\"]\n",
    "\n",
    "rows.append(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    format_word_importances(tokens, attributions)\n",
    "                    ,\n",
    "                    \"<tr>\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "dom.append(\"\".join(rows))\n",
    "dom.append(\"</table>\")\n",
    "display(HTML(\"\".join(dom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
