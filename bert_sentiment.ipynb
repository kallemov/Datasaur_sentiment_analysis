{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed_val = 1\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in polarity_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)} \\t {round(len(y_preds[y_preds==label])/len(y_true),2)}\\n')\n",
    "        \n",
    "        \n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "def predict(dataloader, num_classes):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    total_score=[0]*num_classes\n",
    "    for batch in dataloader:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            scores=torch.max(torch.nn.Softmax(dim=1)(outputs[0]),axis=1)\n",
    "            for i,x in enumerate(scores[0]):\n",
    "                ind=int(scores[1][i])\n",
    "                predictions.append((ind,round(float(x),4)))\n",
    "                total_score[ind] +=x\n",
    "    return predictions, total_score\n",
    "    \n",
    "def evaluate_single(sentense):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    [sentense], \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    encoded_data_test.to(device)\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded_data_test)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  positive\n",
       "1  Reading my kindle2...  Love it... Lee childs i...  positive\n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  positive\n",
       "3  @kenburbary You'll love your Kindle2. I've had...  positive\n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/polarity3_data/twitter_polarity3.csv', encoding='latin-1')\n",
    "#df.set_index('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=df.sample(n=10000,axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_dict={'positive':0, 'negative':1, 'neutral':2}\n",
    "df['label'] = df.sentiment.replace(polarity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text sentiment  label\n",
       "0    @stellargirl I loooooooovvvvvveee my Kindle2. ...  positive      0\n",
       "1    Reading my kindle2...  Love it... Lee childs i...  positive      0\n",
       "2    Ok, first assesment of the #kindle2 ...it fuck...  positive      0\n",
       "3    @kenburbary You'll love your Kindle2. I've had...  positive      0\n",
       "4    @mikefish  Fair enough. But i have the Kindle2...  positive      0\n",
       "..                                                 ...       ...    ...\n",
       "493  Ask Programming: LaTeX or InDesign?: submitted...   neutral      2\n",
       "494  On that note, I hate Word. I hate Pages. I hat...  negative      1\n",
       "495  Ahhh... back in a *real* text editing environm...  positive      0\n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...  negative      1\n",
       "497  Reading the tweets coming out of Iran... The w...  negative      1\n",
       "\n",
       "[498 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df.text.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.10, \n",
    "                                                  random_state=1, \n",
    "                                                  stratify=df.label.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=300, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_val,\n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=300, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_val)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(polarity_dict),\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "model.to(device)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('checkpoints') os.mkdir('checkpoints')\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    \n",
    "    torch.save(model.state_dict(), f'checkpoints/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: positive\n",
      "Accuracy: 16/18 \t 0.89\n",
      "\n",
      "Class: negative\n",
      "Accuracy: 14/18 \t 0.78\n",
      "\n",
      "Class: neutral\n",
      "Accuracy: 11/14 \t 0.79\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.73      0.89      0.80        18\n",
      "    negative       0.93      0.78      0.85        18\n",
      "     neutral       0.85      0.79      0.81        14\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.84      0.82      0.82        50\n",
      "weighted avg       0.83      0.82      0.82        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "report=classification_report(labels_flat, preds_flat, target_names=polarity_dict.keys())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "cm = confusion_matrix(labels_flat, preds_flat)\n",
    "df_cm = pd.DataFrame(cm, index=polarity_dict.keys(), columns=polarity_dict.keys())\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def reset_gpu():\n",
    "    try:\n",
    "        del encoded_data_test\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del output\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del scores, dic\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.memory_reserved())\n",
    "gc.collect()\n",
    "reset_gpu()\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878142464 1438646272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('notebook_data/finetuned_BERT_epoch_10.model', map_location=torch.device('cuda')))\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.memory_reserved())\n",
    "#reset_gpu()\n",
    "#print(torch.cuda.memory_allocated(), torch.cuda.memory_reserved())\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=['Imagine all the people sharing all the world',\n",
    "        'I love this restaurant but the line is too long',\n",
    "        'we should be making a profit by this time next year.',\n",
    "        'She earned her PhD in physics before becoming a postdoctoral fellow at Rockefeller University, where she worked on developing and implementing an underwater touchscreen for dolphins.',\n",
    "        'I did smile today!',\n",
    "        'Applies the Softmax function to an n-dimensional input Tensor', \n",
    "        'I have not slept enough today', \n",
    "        'the weather is crazy hot today',\n",
    "        'Listen Morty, I hate to break it to you, but what people call “love” is just a chemical reaction that compels animals to breed. It hits hard, Morty, then it slowly fades, leaving you stranded in a failing marriage. I did it. Your parents are gonna do it. Break the cycle, Morty. Rise above. Focus on science.',\n",
    "        'I am a scientist, because I invent, transform, create, and destroy for a living, and when I do not like something about the world, I change it',\n",
    "        'I turned myself into a pickle, Morty! I’m Pickle Rick!',\n",
    "        'Do you wanna develop an app?',\n",
    "        'Honey, stop raising your father cholesterol so you can take a hot funeral selfie.',\n",
    "        'To live is to risk it all; otherwise you are just an inert chunk of randomly assembled molecules drifting wherever the universe blows you...'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    inputs, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoded_data_test.to(device)\n",
    "output=model(**encoded_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=torch.max(torch.nn.Softmax(dim=1)(output[0]),axis=1)\n",
    "dic={0:'positive',1:'negative',2:'neutral'}\n",
    "[(dic[int(scores[1][i])],round(float(x),3), inputs[i][:50]+ ('...' if len(inputs[i])>50 else '')) for i,x in enumerate(scores[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  inputs[-2],\n",
    "  max_length=100,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=True,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('test.json',orient='split')\n",
    "#df=pd.concat([jdf,pd.DataFrame(jdf['sentence'].to_list())],axis=1).drop('sentence',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('test.json')\n",
    "df=pd.DataFrame(df['sentences'].to_list())\n",
    "inputs=df.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    df.text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoded_data_test.to(device)\n",
    "output=model(**encoded_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores=torch.max(torch.nn.Softmax(dim=1)(output[0]),axis=1)\n",
    "dic={0:'positive',1:'negative',2:'neutral'}\n",
    "[(dic[int(scores[1][i])],round(float(x),3), inputs[i][:50]+ ('...' if len(inputs[i])>50 else '')) for i,x in enumerate(scores[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "encoded_data_predict = tokenizer.batch_encode_plus(\n",
    "    inputs,\n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=300, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "input_ids_predict = encoded_data_predict['input_ids']\n",
    "attention_masks_predict = encoded_data_predict['attention_mask']\n",
    "\n",
    "dataset_predict = TensorDataset(input_ids_predict, attention_masks_predict)\n",
    "dataloader_predict = DataLoader(dataset_predict, \n",
    "                                sampler=SequentialSampler(dataset_predict), \n",
    "                                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(dataloader_predict,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
